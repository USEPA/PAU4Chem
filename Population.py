import pandas as pd
import numpy as np
from scipy.stats import lognorm
import os, bisect
from Building_PCUs_DB import *
import re

def Calling_US_census(dir_path):
    # 2008 Annual Survey of Manufactures (ASM):
    # Link: https://www.census.gov/data/tables/2008/econ/asm/2008-asm.html
    path_ASM_2008 = dir_path + '/US_Census_Bureau/ASM_2008.xlsx'
    df_ASM_2008 = pd.read_excel(path_ASM_2008, sheet_name = 'ASM_2008',
                        usecols = ['NAICS code',
                                'Year', 'Total value of shipments ($1,000)',
                                'Relative standard error of total value of shipments (%)'],
                        dtype = {'NAICS code': 'object'})
    df_ASM_2008 = df_ASM_2008[df_ASM_2008['Year'] == 2008]
    df_ASM_2008.drop(columns = ['Year'], inplace = True)
    df_ASM_2008['NAICS code'] = df_ASM_2008['NAICS code'].apply(lambda x: str(x).strip())
    # 2008 SUSB Annual Datasets by Establishment Industry
    # Link: https://www.census.gov/data/datasets/2008/econ/susb/2008-susb.html
    path_SUSB_2008 =  dir_path + '/US_Census_Bureau/SUSB_2008.csv'
    df_SUSB_2008 = pd.read_csv(path_SUSB_2008, usecols = ['NAICS code', 'ESTB', 'ENTRSIZEDSCR'],
                       dtype = {'NAICS code': 'object'})
    df_SUSB_2008['NAICS code'] = df_SUSB_2008['NAICS code'].apply(lambda x: str(x).strip())
    df_SUSB_2008 = df_SUSB_2008[df_SUSB_2008['NAICS code'].str.contains(r'^3[123]', na = False)]
    df_SUSB_2008 = df_SUSB_2008[df_SUSB_2008['ENTRSIZEDSCR'] == 'Total']
    df_SUSB_2008.drop(columns = ['ENTRSIZEDSCR'], inplace = True)
    # Merging
    Merged = pd.merge(df_SUSB_2008, df_ASM_2008, on = 'NAICS code', how = 'inner')
    Merged[['Total value of shipments ($1,000)',
            'Relative standard error of total value of shipments (%)']] = \
                Merged[['Total value of shipments ($1,000)',
                        'Relative standard error of total value of shipments (%)']]\
                .applymap(float)
    Merged[['Mean value of shipments ($1,000)', 'SD value of shipments ($1,000)']] = \
                        Merged.apply(lambda x:
                            pd.Series([x.values[2]/x.values[1],
                                       x.values[3]*x.values[2]/(100*x.values[1]**0.5)]),
                            axis = 1)
    Merged = Merged[['NAICS code',
                     'Mean value of shipments ($1,000)',
                     'SD value of shipments ($1,000)']]
    return Merged


def Probability_establishments_within_cluster(naics, establishment, df):
    values = {'Sector': 2,
                'Subsector': 3,
                'Industry Group': 4,
                'NAICS Industry': 5}
    df_interest = df.loc[df['NAICS code'] == naics]
    if df_interest.empty:
        PCU_class = PCU_DB(2008)
        df['NAICS structure'] = df['NAICS code'].apply(lambda x: PCU_class._searching_naics(x, naics))
        df['NAICS structure'] = df['NAICS structure'].map(values)
        Max =  df['NAICS structure'].max()
        df_interest = df[df['NAICS structure'] == Max]
    mean = df_interest['Mean value of shipments ($1,000)'].iloc[0]
    sd = df_interest['SD value of shipments ($1,000)'].iloc[0]
    # measure-of-size (MOS) (e.g., value of shipments, number of employees, etc.),
    # which was highly correlated with pollution abatement operating costs
    # Method of moments
    mu = np.log(mean**2/(sd**2 + mean**2)**0.5)
    theta_2 = np.log(sd**2/mean**2 + 1)
    MOS = lognorm.rvs(s = theta_2**0.5,
                   scale = np.exp(mu),
                   size = int(establishment))
    Best = max(MOS)
    Worst = min(MOS) - 10**(np.log10(min(MOS)) - 2) # For avoiding 0 probability
    MOS.sort()
    # High values of MOS represent a possible high value of PAA. Establishments with high values of PAOC and PACE had a probability of 1 of being selected
    MOS_std = {str(idx + 1):[(val - Worst)/(Best - Worst), val*10**3] for idx, val in enumerate(MOS)}
    return MOS_std


def Probability_cluster_being_sampled(naics, establishment, total_establishments, n_clusters, df_census, df_tri):
    values = {'Sector': 2,
                'Subsector': 3,
                'Industry Group': 4,
                'NAICS Industry': 5}
    df_interest = df_tri.loc[df_tri['NAICS code'] == naics]
    if df_interest.empty:
        PCU_class = PCU_DB(2008)
        df_tri['NAICS structure'] = df_tri['NAICS code'].apply(lambda x: PCU_class._searching_naics(x, naics))
        df_tri['NAICS structure'] = df_tri['NAICS structure'].map(values)
        Max =  df_tri['NAICS structure'].max()
        df_interest = df_tri[df_tri['NAICS structure'] == Max]
        P_cluster = df_interest['% establishments without PAA'].mean()
    else:
        P_cluster = df_interest['% establishments without PAA'].iloc[0]
    Pro_establishment = Probability_establishments_within_cluster(naics, establishment, df_census)
    Pro_establishment_accumulated = dict()
    Shipment_value_establishment = dict()
    sum = 0.0
    for key, val in Pro_establishment.items():
        sum = sum + val[0]
        Pro_establishment_accumulated.update({key: [sum, val[0]]})
        Shipment_value_establishment.update({key: val[1]})
    return pd.Series([P_cluster/100, Pro_establishment_accumulated, Shipment_value_establishment])


def calling_TRI_for_prioritization_sectors(dir_path):
    # The survey prioritized the clusters based on PACE 1994
    Columns = ['ON-SITE - TOTAL WASTE MANAGEMENT',
               'ON-SITE - TOTAL LAND RELEASES',
               'PRIMARY NAICS CODE',
               'TRIFID', 'UNIT OF MEASURE']
    df_TRI_1994 = pd.read_csv(dir_path + '/Ancillary/US_1a_1994.csv',
                                low_memory = False, usecols =  Columns,
                                dtype = {'PRIMARY NAICS CODE': 'object'})
    df_TRI_1994 = df_TRI_1994[df_TRI_1994['PRIMARY NAICS CODE'].str.contains(r'^3[123]', na = False)]
    Flow_columns = ['ON-SITE - TOTAL WASTE MANAGEMENT',
               'ON-SITE - TOTAL LAND RELEASES']
    df_TRI_1994.loc[df_TRI_1994['UNIT OF MEASURE'] == 'Pounds', Flow_columns] *= 0.453592
    df_TRI_1994.loc[df_TRI_1994['UNIT OF MEASURE'] == 'Grams', Flow_columns] *= 10**-3
    df_TRI_1994 = df_TRI_1994.groupby(['TRIFID', 'PRIMARY NAICS CODE'],
                                        as_index = False).sum()
    df_TRI_1994['Any pollution abatement?'] = 'No'
    df_TRI_1994.loc[(df_TRI_1994[Flow_columns] != 0.0).any(axis = 1),\
                    'Any pollution abatement?'] = 'Yes'
    df_TRI_1994.drop(columns = ['TRIFID'] + Flow_columns,
                    inplace = True)
    df_TRI_1994.rename(columns = {'PRIMARY NAICS CODE': 'NAICS code'},
                       inplace = True)
    df_TRI_1994['Number of establishments'] = 1
    df_TRI_1994 = df_TRI_1994.groupby(['Any pollution abatement?', 'NAICS code'],
                        as_index = False).sum()
    df_TRI_1994['Number of establishments in cluster'] = \
                df_TRI_1994.groupby('NAICS code',
                                    as_index =  False)['Number of establishments']\
                            .transform('sum')
    df_TRI_1994 = df_TRI_1994[df_TRI_1994['Any pollution abatement?'] == 'No']
    df_TRI_1994.drop(columns = ['Any pollution abatement?'], inplace = True)
    df_TRI_1994['% establishments without PAA'] = df_TRI_1994[['Number of establishments', \
                                                            'Number of establishments in cluster']] \
                                                .apply(lambda x: 100*x.values[0]/x.values[1],
                                                        axis = 1)
    return df_TRI_1994


def Organizing_sample(n_sampled_establishments, dir_path):
    sampled_clusters = pd.read_csv(dir_path + '/US_Census_Bureau/Selected_clusters_2005.txt',
                                   header=None, index_col=False)
    sampled_clusters = [str(val) for val in sampled_clusters.iloc[:, 0]]
    n_sampled_clusters = len(sampled_clusters)
    # Statistics of U.S. Businesses - Survey 2005
    # Note: 1. The PAOC and PACE only have information for establishments with greather or equal to 20 employees
    #       2. The PAOC and PACE are on establishments
    #       3. The industry sectors surveyed were NAICS codes 31-33
    # Source: https://www.census.gov/prod/2008pubs/ma200-05.pdf
    df_SUSB_2005 = pd.read_csv(dir_path + '/US_Census_Bureau/Statistics_of_US_businesses_2004.csv',
                               low_memory=False, header=None,
                               usecols=[1, 4, 11],
                               names=['NAICS code', 'Establishments (employees >= 20)', 'Employment size'])
    df_SUSB_2005 = df_SUSB_2005[df_SUSB_2005['NAICS code'].str.contains(r'^3[123]')]
    df_SUSB_2005['Establishments (employees >= 20)'] = pd.to_numeric( \
                            df_SUSB_2005['Establishments (employees >= 20)'],
                            errors='coerce')
    df_SUSB_2005 = df_SUSB_2005[pd.notnull(df_SUSB_2005['Establishments (employees >= 20)'])]
    df_SUSB_2005['Establishments (employees >= 20)'] = \
                df_SUSB_2005['Establishments (employees >= 20)'].astype('int')
    row_names = ['20-99 employees', '100-499 employees', '500 + employees']
    df_SUSB_2005 = df_SUSB_2005[df_SUSB_2005['Employment size'].isin(row_names)]
    df_SUSB_2005.drop(columns=['Employment size'],
                      inplace=True)
    df_SUSB_2005 = df_SUSB_2005.groupby('NAICS code', as_index=False).sum()
    df_SUSB_2005 = df_SUSB_2005.loc[df_SUSB_2005['NAICS code'].isin(sampled_clusters)]
    N_total_establishments = df_SUSB_2005['Establishments (employees >= 20)'].sum()
    # Calling information from 1994 TRI
    df_TRI_1994 = calling_TRI_for_prioritization_sectors(dir_path)
    # Calling information from 2008 census
    df_CENSUS_2008 = Calling_US_census(dir_path)
    df_SUSB_2005[['P-cluster', 'P-establishment', 'Shipment-establishment']] = \
            df_SUSB_2005.apply(lambda x: Probability_cluster_being_sampled(
                                            x.values[0], x.values[1],
                                            N_total_establishments,
                                            n_sampled_clusters,
                                            df_CENSUS_2008,
                                            df_TRI_1994),
                               axis=1)
    df_SUSB_2005.sort_values(by=['P-cluster'], inplace=True)
    df_SUSB_2005 = df_SUSB_2005.reset_index()
    df_SUSB_2005['P-cluster accumulated'] = df_SUSB_2005['P-cluster'].cumsum()
    Accumulated = df_SUSB_2005['P-cluster accumulated'].max()
    List_aux = df_SUSB_2005['P-cluster accumulated'].tolist()
    NAICS_list = list()
    P_cluster_list = list()
    Establishment_list = list()
    MOS_list = list()
    P_selected_list = list()
    Shipment_list = list()
    n_sampled = 0
    while n_sampled < n_sampled_establishments:
        P_selected = 0
        # The PACE survey demanded a minimum probability of 0.05
        while P_selected <= 0.05:
            rnd_cluster = np.random.uniform(0, Accumulated)
            idx = bisect.bisect_left(List_aux, rnd_cluster)
            naics = df_SUSB_2005['NAICS code'].iloc[idx]
            P_cluster =  df_SUSB_2005['P-cluster'].iloc[idx]
            List_estab_value = [val[0] for val in list(df_SUSB_2005['P-establishment'].iloc[idx].values())]
            List_estab_MOS = [val[1] for val in list(df_SUSB_2005['P-establishment'].iloc[idx].values())]
            List_estab_shipment = [val for val in list(df_SUSB_2005['Shipment-establishment'].iloc[idx].values())]
            List_estab_number = list(df_SUSB_2005['P-establishment'].iloc[idx].keys())
            rnd_establishment = np.random.uniform(0, max(List_estab_value))
            pos = bisect.bisect_left(List_estab_value, rnd_establishment)
            key = List_estab_number[pos]
            MOS = List_estab_MOS[pos]
            Shipment = List_estab_shipment[pos]
            # P(selected) = P(select in cluster)*P(cluster be selected) they are independent events
            P_selected = MOS*P_cluster
        Pro_establishment_accumulated = dict()
        Shipment_value_establishment = dict()
        sum = 0.0
        for p, v in enumerate(List_estab_MOS):
            if p != pos:
                sum = sum + v
                Pro_establishment_accumulated.update({List_estab_number[p]: [sum, v]})
                Shipment_value_establishment.update({List_estab_number[p]: List_estab_shipment[p]})
        df_SUSB_2005.iloc[idx]['P-establishment'] = Pro_establishment_accumulated
        df_SUSB_2005.iloc[idx]['Shipment-establishment'] = Shipment_value_establishment
        NAICS_list.append(naics)
        P_cluster_list.append(P_cluster)
        P_selected_list.append(P_selected)
        Establishment_list.append(key)
        MOS_list.append(MOS)
        Shipment_list.append(Shipment)
        n_sampled = n_sampled + 1
    df_result = pd.DataFrame({'NAICS code': NAICS_list,
                            'P-cluster': P_cluster_list,
                            'Establishment': Establishment_list,
                            'P-in-cluster': MOS_list,
                            'P-selected': P_selected_list,
                            'Total shipment establishment': Shipment_list})
    # Inflation rate from 2008 to 2020 is 19.09%:
    df_result['Total shipment establishment'] = df_result['Total shipment establishment']*1.1909
    df_result['Unit'] = 'USD'
    print(f'Total value of shipments for NAICS 31-33 in 2008 5,486,265,506,000 USD')
    print(f'Contibution of the selected establishments to the total value of shipments [%]: ', round(100*df_result['Total shipment establishment'].sum()/(5486265506000*1.1909), 4))
    return df_result


def searching_census(naics, media, activity, df):
    values = {'Sector': 2,
             'Subsector': 3,
             'Industry Group': 4,
             'NAICS Industry': 5}
    df = df.loc[df['NAICS code'].apply(lambda x: True if len(x) <= 5 else False)]
    PCU_class = PCU_DB(2008)
    df['NAICS structure'] = df['NAICS code'].apply(lambda x: PCU_class._searching_naics(x, naics))
    df['NAICS structure'] = df['NAICS structure'].map(values)
    Max =  df['NAICS structure'].max()
    if Max == 2:
        df = df[df['NAICS code'] == '31–33']
    else:
        df = df[df['NAICS structure'] == Max]
        df['Length'] = df['NAICS code'].str.len()
        df = df.loc[df.groupby(['Activity', 'Media'])['Length'].idxmin()]
    df =  df.loc[(df['Activity'] == activity) & \
                 (df['Media'] == media)]
    dictionary = {col1: col2 for col1 in ['RSE', ' activity & media', 'P-media', 'P-activity', \
                                        'Total PA', 'Total s', 'Info'] \
                                        for col2 in df.columns if col1 in col2}
    s =  pd.Series([df[col].values for col in dictionary.values()])
    return s


def mean_standard(establishments, shipment_flow, rse, total, shipment, confidence):
    if total >= shipment:
        ratio = 0.0044
    else:
        ratio = total/shipment
    try:
        total = ratio*shipment_flow
        Mean = total/establishments
        SD = (rse*total/(100*(establishments)**0.5))
        CI = [Mean - confidence*SD/(establishments)**0.5,
              Mean + confidence*SD/(establishments)**0.5]
        return pd.Series([total, Mean, SD, CI])
    except KeyError:
        return pd.Series([None]*4)


def selecting_establishment_by_activity_and_media(info_establishments, probability_activity, probability_media):
    # P(activity and media and establishment) =  P(activity and media)*P(establishment). They are independent events
    selected_establishments = 0
    Info_selected_establishments = dict()
    for estab, info in info_establishments.items():
        rnd_a = np.random.rand()
        if probability_activity > rnd_a:
            rnd_m = np.random.rand()
            if probability_media > rnd_m:
                probability_establishment = info[1]
                rnd_establishment = np.random.rand()
                if probability_establishment > rnd_establishment:
                    selected_establishments = selected_establishments + 1
                    Info_selected_establishments.update({estab: info})
    if probability_activity*probability_media != 0 and selected_establishments == 0:
        selected_establishments = 1
        maximum = max([val[0] for val in info_establishments.values()])
        Info_selected_establishments.update({key: val for key, val in \
                                        info_establishments.items() if val[0] == maximum})
    return pd.Series([selected_establishments, Info_selected_establishments])


def estimating_mass_by_activity_and_media(mu, theta_2, info_establishments, P_media, P_activity):
    prob = [1 - vals[1]*P_activity*P_media for vals in info_establishments.values()] # 1 - cdf
    shipments = [vals[0] for vals in info_establishments.values()]
    try:
        vals = lognorm.isf(prob,
                           s = theta_2**0.5,
                           scale = np.exp(mu))
    except ValueError:
        vals = lognorm.isf(prob,
                           s = 10**-9,
                           scale = np.exp(mu))
    shipment_mass = sum([shipments[idx]/val for idx, val in enumerate(vals)])
    total_mass = sum(vals)
    total_shipment = sum(shipments)
    return pd.Series([total_mass, total_shipment, shipment_mass])


def searching_establishments_by_hierarchy(naics, df):
    try:
        if (naics == '31–33') | (len(naics) == 3):
            naics = '3[123]'
            n = '2,4'
        elif len(naics) == 4:
            n = '1,2'
        elif len(naics) == 5:
            n = '1'
        regex =  re.compile(r'^%s[0-9]{%s}' % (naics, n))
        df_result = df.loc[df['NAICS code'].apply(lambda x: True if re.match(regex, x) else False)]
        if not df_result.empty:
            estab = int(df_result['Establishments (employees >= 20)'].sum())
            shipment = df_result['Total shipment'].sum()
            Dictionary_establishments =  dict()
            i =  0
            for idx, row in df_result.iterrows():
                for values in row['Info establishments'].values():
                    i = i + 1
                    Dictionary_establishments.update({str(i): values})
            return pd.Series([estab, shipment, Dictionary_establishments])
        else:
            return pd.Series([None]*3)
    except UnboundLocalError:
        return pd.Series([None]*3)


def normalizing_shipments(df):
    def function_aux(info_estab, P_m, P_a, P_on, estab, Total):
        vals = info_estab[estab]
        info_estab.update({estab: [vals[0]*P_m*P_a*P_on*0.01/Total, vals[1]]})
        return info_estab
    estabs = set([key for idx, row in df.iterrows()\
                          for key in row['Info probable establishments'].keys()])
    for estab in estabs:
        idx = df.loc[df['Info probable establishments'].apply(lambda x: \
                        True if estab in x.keys() else False)].index.tolist()
        Total =  (df.loc[idx, 'P-media']*df.loc[idx, 'P-activity']).sum()
        df.loc[idx]['Info probable establishments'] = \
                df.loc[idx][['Info probable establishments',\
                             'P-media',\
                             'P-activity',\
                             '% On-site flow']]\
                        .apply(lambda x: function_aux(x.values[0],\
                                                      x.values[1],\
                                                      x.values[2],\
                                                      x.values[3],\
                                                      estab,\
                                                      Total),\
                                axis = 1)
    return df


if __name__ == '__main__':
    dir_path = os.path.dirname(os.path.realpath(__file__))
    df_result = Organizing_sample(20378, dir_path)
    df_result.to_csv(dir_path + '/US_Census_Bureau/Sampled_establishments.csv', index = False)
